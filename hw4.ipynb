{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import struct\n",
    "import requests\n",
    "import cPickle\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.cluster import KMeansClusterer,cosine_distance\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fv=brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/weiweili/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "rfv = [w for w in fv if w.lower() not in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "new_rfv=[]\n",
    "for token in rfv: \n",
    "    new_token = regex.sub(u'', token)\n",
    "    if not new_token == u'':\n",
    "        new_rfv.append(new_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rfv=[x.lower() for x in new_rfv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(w.lower() for w in new_rfv)\n",
    "fdistV=fdist.most_common(5000)\n",
    "fdistC=fdist.most_common(1000)\n",
    "V=[word for (word, count) in fdistV]\n",
    "C=[word for (word, count) in fdistC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=np.zeros([len(V),len(C)])\n",
    "for i in range(len(V)):\n",
    "    if i % 500 == 0:\n",
    "        print i\n",
    "    text_pos=[pos for pos, word in enumerate(new_rfv) if word==V[i]]\n",
    "    for k in range(len(text_pos)):\n",
    "        if (text_pos[k]>=2 and (text_pos[k]+2-len(new_rfv)+1)<=0):\n",
    "            window=set([new_rfv[text_pos[k]-2],new_rfv[text_pos[k]-1],new_rfv[text_pos[k]+1],new_rfv[text_pos[k]+2]])\n",
    "            #w=[word for word in C if word in window]\n",
    "            C_pos=[pos for pos, word in enumerate(C) if word in window]\n",
    "        elif(text_pos[k]>=2 and (text_pos[k]+2-len(new_rfv)+1)==1):\n",
    "            window=set([new_rfv[text_pos[k]-2],new_rfv[text_pos[k]-1],new_rfv[text_pos[k]+1]])\n",
    "            #w=[word for word in C if word in window]\n",
    "            C_pos=[pos for pos, word in enumerate(C) if word in window]\n",
    "        elif (text_pos[k]>=2 and (text_pos[k]+2-len(new_rfv)+1)==2):\n",
    "            window=set([new_rfv[text_pos[k]-2],new_rfv[text_pos[k]-1]])\n",
    "            #w=[word for word in C if word in window]\n",
    "            C_pos=[pos for pos, word in enumerate(C) if word in window]\n",
    "        elif (text_pos[k]==1):\n",
    "            window=set([new_rfv[text_pos[k]-1],new_rfv[text_pos[k]+1],new_rfv[text_pos[k]+2]])\n",
    "            #w=[word for word in C if word in window]\n",
    "            C_pos=[pos for pos, word in enumerate(C) if word in window]\n",
    "        elif (text_pos[k]==0):\n",
    "            window=set([new_rfv[text_pos[k]+1],new_rfv[text_pos[k]+2]])\n",
    "            #w=[word for word in C if word in window]\n",
    "            C_pos=[pos for pos, word in enumerate(C) if word in window]\n",
    "        n[i][C_pos]+=1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('nwc_hw4.txt','wb') as f:\n",
    "    pickle.dump(n,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "with open('nwc_hw4.txt','rb') as f2:\n",
    "    nwc=pickle.load(f2)\n",
    "print(nwc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 65.,  71.,  44., ...,   0.,   1.,   1.],\n",
       "       [ 71.,  68.,  66., ...,   2.,   1.,   4.],\n",
       "       [ 41.,  64.,  30., ...,   0.,   0.,   1.],\n",
       "       ..., \n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "       [  1.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vcount=[count for (word, count) in fdistV]\n",
    "ni=np.sum(nwc, axis=1) #5000*1\n",
    "ind_zero=[pos for pos,n in enumerate(ni) if n==0]\n",
    "ni[ind_zero]=1\n",
    "Pcw=nwc/ni[:,None] #5000*1000\n",
    "Pc=np.sum(nwc,axis=0)/nwc.sum() #1000*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsion = 1e-12\n",
    "tmp=(Pcw+epsion)/(Pc+epsion)\n",
    "tmp=np.log(tmp)\n",
    "fcw=np.maximum(0,tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##PCA\n",
    "pca = PCA(n_components=100)\n",
    "pca_fcw=pca.fit_transform(fcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Nearest neighbor results\n",
    "words=['communism','autumn','cigarette','pulmonary', 'mankind', 'africa', 'chicago', 'revolution', 'september',\n",
    "'chemical', 'detergent', 'dictionary', 'storm', 'worship', 'head','brown','business','face','money','air',\n",
    "      'month', 'third','seven','parents','happy']\n",
    "\n",
    "ind_words=[pos for pos, word in enumerate(V) if word in words ]\n",
    "\n",
    "tmp=[]\n",
    "for i in range(len(words)):\n",
    "    tmp.append(V[ind_words[i]])\n",
    "words=tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist=np.zeros([len(V),len(words)])\n",
    "for i in range(len(words)):\n",
    "    dist[:,i]=1-np.dot(pca_fcw,pca_fcw[ind_words[i],:])/(np.linalg.norm(pca_fcw,axis=1)* \\\n",
    "                                                        np.linalg.norm(pca_fcw[ind_words[i],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind_mindist=dist.argsort(axis=0)[1]\n",
    "nnWords=[]\n",
    "for i in range(len(ind_mindist)):\n",
    "    nnWords.append(V[ind_mindist[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'hands', u'local', u'eyes', u'pay', u'water', u'second', u'hair', u'last', u'ten', u'children', u'couldnt', u'kay', u'reflection', u'weapons', u'instances', u'text', u'kay', u'kay', u'nation', u'theological', u'artery', u'eighteenth', u'slid', u'fabrics', u'winter']\n"
     ]
    }
   ],
   "source": [
    "print nnWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'head', u'business', u'face', u'money', u'air', u'third', u'brown', u'month', u'seven', u'parents', u'happy', u'chicago', u'communism', u'revolution', u'chemical', u'dictionary', u'september', u'africa', u'mankind', u'worship', u'pulmonary', u'storm', u'cigarette', u'detergent', u'autumn']\n"
     ]
    }
   ],
   "source": [
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=100, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## K-means clustering \n",
    "model = KMeans(n_clusters=100)\n",
    "model.fit(pca_fcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "center=model.predict(pca_fcw)  #100  ##center for every word in V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind_clusters=[[] for i in range(100)]\n",
    "import itertools\n",
    "for i in range(100):\n",
    "    a=np.where(center==i)\n",
    "    ind_clusters[i]=list(itertools.chain.from_iterable(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters=[[] for i in range(100)]\n",
    "for i in range(100):\n",
    "    clusters[i]=[word for pos, word in enumerate(V) if pos in ind_clusters[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##nltk kmeans cosine_distance\n",
    "\n",
    "from nltk.cluster import KMeansClusterer,cosine_distance\n",
    "clusterer2=KMeansClusterer(100, distance=cosine_distance, \n",
    "                      repeats=5,avoid_empty_clusters=True)\n",
    "clusters2 = clusterer2.cluster(pca_fcw, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 0 99\n"
     ]
    }
   ],
   "source": [
    "print len(clusters2), min(clusters2),max(clusters2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##nltk kmeans euclidean_distance\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "clusterer = KMeansClusterer(100, euclidean_distance, repeats=5,avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(pca_fcw, True)  #center for every word in V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 0 99\n"
     ]
    }
   ],
   "source": [
    "print len(clusters), min(clusters),max(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#euclidean_distance:  index of every center in clusters, words in that cluster\n",
    "ind_cluster=[pos for pos, center in enumerate(clusters) if center==2]\n",
    "word_cluster=[word for pos, word in enumerate(V) if pos in ind_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'capacity', u'economy', u'initial', u'dramatic', u'machinery', u'critical', u'developing', u'advertising', u'mere', u'accomplished', u'varied', u'examine', u'extend', u'complicated', u'economical', u'urgent', u'expanded']\n"
     ]
    }
   ],
   "source": [
    "print word_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cosine_distance:  index of every center in clusters, words in that cluster\n",
    "ind_cluster2=[pos for pos, center in enumerate(clusters2) if center==29]\n",
    "word_cluster2=[word for pos, word in enumerate(V) if pos in ind_cluster2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_cluster2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'speak', u'theres', u'exactly', u'suppose', u'begin', u'believed', u'interior', u'capable', u'object', u'dream', u'save', u'minds', u'remarks', u'knowing', u'surely', u'sold', u'promise', u'odd', u'saved', u'happens', u'discover', u'definite', u'arent', u'accurate', u'missing', u'examine', u'suspect', u'happening', u'meaningful', u'settle', u'intimate']\n"
     ]
    }
   ],
   "source": [
    "print word_cluster2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
